{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a148276e-ea1f-4b2b-9659-f6e09ed5564a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five rows of the dataset:\n",
      "         Date   Price    Open    High     Low  Vol. Change %\n",
      "0  07/19/2024  171.30  171.48  171.90  170.90   NaN   -0.08%\n",
      "1  07/18/2024  171.44  170.82  171.60  169.98   NaN    0.38%\n",
      "2  07/17/2024  170.79  172.59  172.86  170.69   NaN   -1.01%\n",
      "3  07/16/2024  172.54  172.17  172.96  172.13   NaN    0.23%\n",
      "4  07/15/2024  172.14  172.20  172.57  171.57   NaN   -0.03%\n",
      "\n",
      "Last five rows of the dataset:\n",
      "            Date   Price    Open    High     Low  Vol. Change %\n",
      "6397  01/07/2000  108.44  108.67  109.03  107.74   NaN   -0.20%\n",
      "6398  01/06/2000  108.66  107.65  109.31  107.14   NaN    0.99%\n",
      "6399  01/05/2000  107.59  106.34  107.75  105.65   NaN    1.04%\n",
      "6400  01/04/2000  106.48  104.02  106.60  103.92   NaN    2.10%\n",
      "6401  01/03/2000  104.29  102.66  104.39  102.07   NaN    1.22%\n",
      "Before preprocessing:\n",
      "         Date   Price    Open    High     Low  Vol. Change %\n",
      "0  07/19/2024  171.30  171.48  171.90  170.90   NaN   -0.08%\n",
      "1  07/18/2024  171.44  170.82  171.60  169.98   NaN    0.38%\n",
      "2  07/17/2024  170.79  172.59  172.86  170.69   NaN   -1.01%\n",
      "3  07/16/2024  172.54  172.17  172.96  172.13   NaN    0.23%\n",
      "4  07/15/2024  172.14  172.20  172.57  171.57   NaN   -0.03%\n",
      "After converting 'Date' column:\n",
      "        Date   Price    Open    High     Low  Vol. Change %\n",
      "0 2024-07-19  171.30  171.48  171.90  170.90   NaN   -0.08%\n",
      "1 2024-07-18  171.44  170.82  171.60  169.98   NaN    0.38%\n",
      "2 2024-07-17  170.79  172.59  172.86  170.69   NaN   -1.01%\n",
      "3 2024-07-16  172.54  172.17  172.96  172.13   NaN    0.23%\n",
      "4 2024-07-15  172.14  172.20  172.57  171.57   NaN   -0.03%\n",
      "After preprocessing:\n",
      "Empty DataFrame\n",
      "Columns: [Price, Open, High, Low, Vol., Change %, Price_diff, Open_diff, High_diff, Low_diff, Change_diff, Vol_diff]\n",
      "Index: []\n",
      "X shape: (0, 10), y shape: (0,)\n",
      "First five rows of the features (X):\n",
      "Empty DataFrame\n",
      "Columns: [Open, High, Low, Vol., Price_diff, Open_diff, High_diff, Low_diff, Vol_diff, Change_diff]\n",
      "Index: []\n",
      "\n",
      "First five rows of the target (y):\n",
      "Series([], Name: Price, dtype: float64)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=0.8, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 143\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# Ensure the script runs only if executed as the main module\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 131\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m X, y \u001b[38;5;241m=\u001b[39m preprocess_data(df)  \u001b[38;5;66;03m# Preprocess the data\u001b[39;00m\n\u001b[1;32m    130\u001b[0m explore_preprocesseddata(X,y)\n\u001b[0;32m--> 131\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Split data into training and testing sets\u001b[39;00m\n\u001b[1;32m    132\u001b[0m results \u001b[38;5;241m=\u001b[39m train_evaluate_model(X_train, X_test, y_train, y_test)  \u001b[38;5;66;03m# Train and evaluate models\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Print the RMSE for each model\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 73\u001b[0m, in \u001b[0;36mtrain_test_split_data\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_test_split_data\u001b[39m(X, y):\n\u001b[1;32m     70\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    This function splits the features (X) and target (y) into training and testing sets.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2778\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2775\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[1;32m   2777\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 2778\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[1;32m   2780\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2782\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m   2783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2408\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2405\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[1;32m   2407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2408\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2409\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2411\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[1;32m   2412\u001b[0m     )\n\u001b[1;32m   2414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=0.8, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd  # for data manipulation and analysis\n",
    "import numpy as np  # for numerical operations\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV  # for splitting the data and hyperparameter tuning\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor  # for machine learning models\n",
    "from sklearn.metrics import mean_squared_error  # for evaluating model performance\n",
    "import xgboost as xgb  # for the XGBoost model\n",
    "\n",
    "# Function to load CSV data\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    This function loads the CSV data from the specified file path.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "def explore_data(df):\n",
    "    # Visualize the first and last five rows of the dataset\n",
    "    print(\"First five rows of the dataset:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nLast five rows of the dataset:\")\n",
    "    print(df.tail())\n",
    "    \n",
    "def explore_preprocesseddata(X,y):    \n",
    "    # Visualize the preprocessed data\n",
    "    print(\"First five rows of the features (X):\")\n",
    "    print(X.head())\n",
    "    print(\"\\nFirst five rows of the target (y):\")\n",
    "    print(y.head())\n",
    "\n",
    "\n",
    "\n",
    "# Function to preprocess the data\n",
    "def preprocess_data(df):\n",
    "    print(\"Before preprocessing:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')  # Convert 'Date' column to datetime\n",
    "    print(\"After converting 'Date' column:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    df.set_index('Date', inplace=True)  # Set 'Date' as the index\n",
    "    \n",
    "    # Create new features based on the difference of consecutive rows\n",
    "    df['Price_diff'] = df['Price'].diff()\n",
    "    df['Open_diff'] = df['Open'].diff()\n",
    "    df['High_diff'] = df['High'].diff()\n",
    "    df['Low_diff'] = df['Low'].diff()\n",
    "    df['Change_diff'] = df['Change %'].str.replace('%', '', regex=False).astype(float).diff()\n",
    "    \n",
    "    # Fill missing 'Vol.' values with 0 and calculate the difference\n",
    "    df['Vol.'] = df['Vol.'].replace('', 0).astype(float)\n",
    "    df['Vol_diff'] = df['Vol.'].diff()\n",
    "\n",
    "    df.dropna(inplace=True)  # Drop rows with any missing values\n",
    "\n",
    "    print(\"After preprocessing:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Define features (X) and target (y)\n",
    "    X = df[['Open', 'High', 'Low', 'Vol.', 'Price_diff', 'Open_diff', 'High_diff', 'Low_diff', 'Vol_diff', 'Change_diff']]\n",
    "    y = df['Price']\n",
    "    \n",
    "    print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Function to split data into training and testing sets\n",
    "def train_test_split_data(X, y):\n",
    "    \"\"\"\n",
    "    This function splits the features (X) and target (y) into training and testing sets.\n",
    "    \"\"\"\n",
    "    return train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to train and evaluate models\n",
    "def train_evaluate_model(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    This function trains and evaluates multiple models:\n",
    "    1. RandomForestRegressor with GridSearchCV for hyperparameter tuning.\n",
    "    2. XGBoostRegressor with GridSearchCV for hyperparameter tuning.\n",
    "    3. BaggingRegressor with the best RandomForest and XGBoost models.\n",
    "    4. Calculates RMSE for each model and returns the results.\n",
    "    \"\"\"\n",
    "    # Train and tune RandomForestRegressor\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    param_grid_rf = {'n_estimators': [100, 200, 300], 'max_depth': [5, 10, 15], 'min_samples_split': [2, 5, 10]}\n",
    "    grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_search_rf.fit(X_train, y_train)\n",
    "    best_rf = grid_search_rf.best_estimator_\n",
    "\n",
    "    # Predict and calculate RMSE for RandomForestRegressor\n",
    "    y_pred_rf = best_rf.predict(X_test)\n",
    "    rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "\n",
    "    # Train and tune XGBoostRegressor\n",
    "    xgb_model = xgb.XGBRegressor(random_state=42)\n",
    "    param_grid_xgb = {'n_estimators': [100, 200, 300], 'max_depth': [5, 10, 15], 'learning_rate': [0.01, 0.1, 0.3]}\n",
    "    grid_search_xgb = GridSearchCV(estimator=xgb_model, param_grid=param_grid_xgb, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_search_xgb.fit(X_train, y_train)\n",
    "    best_xgb = grid_search_xgb.best_estimator_\n",
    "\n",
    "    # Predict and calculate RMSE for XGBoostRegressor\n",
    "    y_pred_xgb = best_xgb.predict(X_test)\n",
    "    rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "\n",
    "    # Train and evaluate BaggingRegressor with RandomForest\n",
    "    bagging_rf = BaggingRegressor(base_estimator=best_rf, n_estimators=10, random_state=42)\n",
    "    bagging_rf.fit(X_train, y_train)\n",
    "    y_pred_bagging_rf = bagging_rf.predict(X_test)\n",
    "    rmse_bagging_rf = np.sqrt(mean_squared_error(y_test, y_pred_bagging_rf))\n",
    "\n",
    "    # Train and evaluate BaggingRegressor with XGBoost\n",
    "    bagging_xgb = BaggingRegressor(base_estimator=best_xgb, n_estimators=10, random_state=42)\n",
    "    bagging_xgb.fit(X_train, y_train)\n",
    "    y_pred_bagging_xgb = bagging_xgb.predict(X_test)\n",
    "    rmse_bagging_xgb = np.sqrt(mean_squared_error(y_test, y_pred_bagging_xgb))\n",
    "\n",
    "    # Calculate final predictions and RMSE for ensemble model\n",
    "    final_pred = (y_pred_bagging_rf + y_pred_bagging_xgb) / 2\n",
    "    final_rmse = np.sqrt(mean_squared_error(y_test, final_pred))\n",
    "\n",
    "    return rmse_rf, rmse_xgb, rmse_bagging_rf, rmse_bagging_xgb, final_rmse\n",
    "\n",
    "# Main function to execute the workflow\n",
    "def main():\n",
    "    file_path = 'forex.csv'  # Path to your CSV file\n",
    "    df = load_data(file_path)  # Load the data\n",
    "    explore_data(df)\n",
    "    X, y = preprocess_data(df)  # Preprocess the data\n",
    "    explore_preprocesseddata(X,y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split_data(X, y)  # Split data into training and testing sets\n",
    "    results = train_evaluate_model(X_train, X_test, y_train, y_test)  # Train and evaluate models\n",
    "\n",
    "    # Print the RMSE for each model\n",
    "    print(f'Random Forest RMSE: {results[0]}')\n",
    "    print(f'XGBoost RMSE: {results[1]}')\n",
    "    print(f'Bagging Random Forest RMSE: {results[2]}')\n",
    "    print(f'Bagging XGBoost RMSE: {results[3]}')\n",
    "    print(f'Ensemble Model RMSE: {results[4]}')\n",
    "\n",
    "# Ensure the script runs only if executed as the main module\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074436ba-0086-4edc-8ff2-2e6bdec2db89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
